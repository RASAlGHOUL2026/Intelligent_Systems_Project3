{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOvvWAVTkMR7"
      },
      "source": [
        "# Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "##  Imports and Setup\n",
        "\n",
        "Let's start with the base imports."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "iPwRWY7SbjFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk4FU-jx9kc3"
      },
      "outputs": [],
      "source": [
        "# This Colab requires TF 2.5.\n",
        "!pip install -U \"tensorflow>=2.5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn5_uV1HLvaz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import cv2\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageOps\n",
        "from six.moves.urllib.request import urlopen\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import math\n",
        "import glob\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "outputs": [],
      "source": [
        "# @title  import models and dats set keypoints\n",
        "\n",
        "def load_image_into_numpy_array(path):\n",
        "  image = None\n",
        "  if(path.startswith('http')):\n",
        "    response = urlopen(path)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    image = Image.open(image_data)\n",
        "  else:\n",
        "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (1, im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "\n",
        "ALL_MODELS = {\n",
        "\n",
        "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
        "'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
        "'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
        "'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
        "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
        "'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
        "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
        "}\n",
        "\n",
        "#List of tuples with Human Keypoints for the COCO 2017 dataset. This is needed for models with keypoints.\n",
        "\n",
        "COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n",
        " (0, 2),\n",
        " (1, 3),\n",
        " (2, 4),\n",
        " (0, 5),\n",
        " (0, 6),\n",
        " (5, 7),\n",
        " (7, 9),\n",
        " (6, 8),\n",
        " (8, 10),\n",
        " (5, 6),\n",
        " (5, 11),\n",
        " (6, 12),\n",
        " (11, 12),\n",
        " (11, 13),\n",
        " (13, 15),\n",
        " (12, 14),\n",
        " (14, 16)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bNk1gzh0TN"
      },
      "source": [
        "## Visualization tools\n",
        "\n",
        "To visualize the images with the proper detected boxes, keypoints and segmentation, we will use the TensorFlow Object Detection API. To install it we will clone the repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oi28cqGGFWnY"
      },
      "outputs": [],
      "source": [
        "# @title  Clone the tensorflow models repository\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "outputs": [],
      "source": [
        "# @title  Intalling the Object Detection API\n",
        "\n",
        "#a cell magic command that allows you to run multiple shell commands as a script\n",
        "%%bash \n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JCeQU3fkayh"
      },
      "outputs": [],
      "source": [
        "# @title  Import the dependencies will be needed later\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKtD0IeclbL5"
      },
      "source": [
        "Load label map data from the repository that we loaded the Object Detection API code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mucYUS6exUJ"
      },
      "outputs": [],
      "source": [
        "#  @title  getting label map\n",
        "# Set the path to the directory containing the label map\n",
        "path_to_labelmap = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(path_to_labelmap, use_display_name=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6917xnUSlp9x"
      },
      "source": [
        "## Build a detection model and load pre-trained model weights\n",
        "\n",
        "Here we will choose which Object Detection model we will use. Select the architecture and it will be loaded automatically. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtwrSqvakTNn"
      },
      "outputs": [],
      "source": [
        "model_display_name = 'Faster R-CNN ResNet50 V1 640x640' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\n",
        "model_handle = ALL_MODELS[model_display_name]\n",
        "\n",
        "print('Selected model:'+ model_display_name)\n",
        "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muhUt-wWL582"
      },
      "source": [
        "## Loading the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBuD07fLlcEO"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    model = hub.load(model_handle)\n",
        "    print('model loaded!')\n",
        "except:\n",
        "    print('model failed to load')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sepration of human and non human images"
      ],
      "metadata": {
        "id": "s9_MQ8TfgsnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Classify, create new folders in existig folders ad sed the images to the new folders\n",
        "\n",
        "#Load the label map\n",
        "category_index = {1: {'id': 1, 'name': 'person'}}\n",
        "\n",
        "# Define the path to the directory containing the images\n",
        "original_dir = '/content/drive/MyDrive/HAZIQ/imagejpeg'\n",
        "\n",
        "# Create two new directories for human and no_human images\n",
        "human_dir = os.path.join(original_dir, 'Human')\n",
        "os.makedirs(human_dir, exist_ok=True)\n",
        "no_human_dir = os.path.join(original_dir, 'No human')\n",
        "os.makedirs(no_human_dir, exist_ok=True)\n",
        "\n",
        "# Loop through each file and load images that have valid image extensions\n",
        "for file in os.listdir(original_dir):\n",
        "    # Exclude directories and non-image files\n",
        "    if not os.path.isdir(os.path.join(original_dir, file)) and \\\n",
        "       os.path.splitext(file)[1].lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "        \n",
        "        # Load the image into a PIL Image object\n",
        "        image_pil = Image.open(os.path.join(original_dir, file))\n",
        "\n",
        "        # Convert the image to a numpy array\n",
        "        image_np = np.array(image_pil)\n",
        "\n",
        "        # Expand the dimensions of the array to make it compatible with the model\n",
        "        image_np = np.expand_dims(image_np, axis=0)\n",
        "        \n",
        "        # Running inference\n",
        "        results = model(image_np)\n",
        "\n",
        "        # Convert the images to NumPy array\n",
        "        result = {key:value.numpy() for key,value in results.items()}\n",
        "\n",
        "        # Filter for person class only\n",
        "        person_mask = (result['detection_classes'][0] == 1)\n",
        "\n",
        "        if person_mask.any():  # Check if at least one person is detected\n",
        "            # Save image to human_dir\n",
        "            shutil.move(os.path.join(original_dir, file), os.path.join(human_dir, file))\n",
        "        else:\n",
        "            # Save image to no_human_dir\n",
        "            shutil.move(os.path.join(original_dir, file), os.path.join(no_human_dir, file))"
      ],
      "metadata": {
        "id": "hZ1RJIlnupxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show the images with human in it\n",
        "\n",
        "# Resizes an image to the specified size and pads the image\n",
        "def resize_and_pad(image, size, pad_color):\n",
        "    # Resize the image while maintaining its aspect ratio\n",
        "    original_size = max(image.size)\n",
        "    new_size = (int(size * image.size[0] / original_size), int(size * image.size[1] / original_size))\n",
        "    image_resized = image.resize(new_size, Image.BICUBIC)\n",
        "\n",
        "    # Create a new image of the desired size with the given color\n",
        "    new_image = Image.new(\"RGB\", (size, size), pad_color)\n",
        "\n",
        "    # Paste the resized image onto the new image, centered\n",
        "    x_offset = (size - new_size[0]) // 2\n",
        "    y_offset = (size - new_size[1]) // 2\n",
        "    new_image.paste(image_resized, (x_offset, y_offset))\n",
        "\n",
        "    return new_image\n",
        "\n",
        "# Define the input size\n",
        "input_size = 640\n",
        "\n",
        "# Define the number of columns per row\n",
        "columns = 3\n",
        "\n",
        "# Check if there are any images in the directory, if not, skip the loop\n",
        "if len(os.listdir(human_dir)) == 0:\n",
        "    print('No images with human found in the directory!')\n",
        "else:\n",
        "    num_images = len(os.listdir(human_dir))\n",
        "    rows = num_images // columns\n",
        "    if num_images % columns != 0:\n",
        "        rows += 1\n",
        "\n",
        "    # Creates a grid of subplots\n",
        "    fig, axes = plt.subplots(rows, columns, figsize=(15, rows*5))\n",
        "\n",
        "    # Adjusts the spacing between subplots\n",
        "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "    # Convert axes to a 2D array if it is 1D\n",
        "    if rows == 1 or columns == 1:\n",
        "        axes = np.array(axes).reshape(rows, columns)\n",
        "    \n",
        "    # Loop through each file in the human directory and display the image\n",
        "    for i, file in enumerate(os.listdir(human_dir)):\n",
        "\n",
        "        # Load the image into a PIL Image object\n",
        "        image_pil = Image.open(os.path.join(human_dir, file))\n",
        "\n",
        "        # Calculate the maximum dimension of the image\n",
        "        max_dimension = max(image_pil.size)\n",
        "\n",
        "        # Resize and pad the image to make it square\n",
        "        resized_image = resize_and_pad(image_pil, input_size, (0, 0, 0))\n",
        "\n",
        "        # Calculate the row and column for this image\n",
        "        row = i // columns\n",
        "        col = i % columns\n",
        "\n",
        "        # Add the image to the plot\n",
        "        ax = axes[row, col]\n",
        "        ax.imshow(resized_image)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Set aspect ratio to square\n",
        "        ax.set_aspect('equal')\n",
        "    \n",
        "    # Adjust the subplot for each row\n",
        "    for i in range(rows):\n",
        "        for j in range(columns):\n",
        "            if j >= num_images % columns and i == rows - 1:\n",
        "                axes[i, j].axis('off')\n",
        "                axes[i, j].set_xticklabels([])\n",
        "                axes[i, j].set_yticklabels([])\n",
        "    \n",
        "    # Set the title of the plot\n",
        "    fig.suptitle('Images with human', fontsize=24)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "7jddJtv2xp-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show the images ***without any human*** in it\n",
        "\n",
        "# Resizes an image to the specified size and pads the image\n",
        "def resize_and_pad(image, size, pad_color):\n",
        "\n",
        "    # Resize the image while maintaining its aspect ratio\n",
        "    original_size = max(image.size)\n",
        "    new_size = (int(size * image.size[0] / original_size), int(size * image.size[1] / original_size))\n",
        "    image_resized = image.resize(new_size, Image.BICUBIC)\n",
        "\n",
        "    # Create a new image of the desired size with the given color\n",
        "    new_image = Image.new(\"RGB\", (size, size), pad_color)\n",
        "\n",
        "    # Paste the resized image onto the new image, centered\n",
        "    x_offset = (size - new_size[0]) // 2\n",
        "    y_offset = (size - new_size[1]) // 2\n",
        "    new_image.paste(image_resized, (x_offset, y_offset))\n",
        "\n",
        "    return new_image\n",
        "\n",
        "# Define the input size\n",
        "input_size = 640\n",
        "\n",
        "# Define the number of columns per row\n",
        "columns = 3\n",
        "\n",
        "# Check if there are any images in the directory, if not, skip the loop\n",
        "if len(os.listdir(no_human_dir)) == 0:\n",
        "    print('No images without human found in the directory!')\n",
        "else:\n",
        "    num_images = len(os.listdir(no_human_dir))\n",
        "    rows = num_images // columns\n",
        "    if num_images % columns != 0:\n",
        "        rows += 1\n",
        "\n",
        "    # Creates a grid of subplots\n",
        "    fig, axes = plt.subplots(rows, columns, figsize=(15, rows*5))\n",
        "\n",
        "    # Adjusts the spacing between subplots\n",
        "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "    # Convert axes to a 2D array if it is 1D\n",
        "    if rows == 1 or columns == 1:\n",
        "        axes = np.array(axes).reshape(rows, columns)\n",
        "    \n",
        "    # Loop through each file in the human directory and display the image\n",
        "    for i, file in enumerate(os.listdir(no_human_dir)):\n",
        "\n",
        "        # Load the image into a PIL Image object\n",
        "        image_pil = Image.open(os.path.join(no_human_dir, file))\n",
        "\n",
        "        # Calculate the maximum dimension of the image\n",
        "        max_dimension = max(image_pil.size)\n",
        "\n",
        "        # Resize and pad the image to make it square\n",
        "        resized_image = resize_and_pad(image_pil, input_size, (0, 0, 0))\n",
        "\n",
        "        # Calculate the row and column for this image\n",
        "        row = i // columns\n",
        "        col = i % columns\n",
        "\n",
        "        # Add the image to the plot\n",
        "        ax = axes[row, col]\n",
        "        ax.imshow(resized_image)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Set aspect ratio to square\n",
        "        ax.set_aspect('equal')\n",
        "    \n",
        "    # Adjust the subplot for each row\n",
        "    for i in range(rows):\n",
        "        for j in range(columns):\n",
        "            if j >= num_images % columns and i == rows - 1:\n",
        "                axes[i, j].axis('off')\n",
        "                axes[i, j].set_xticklabels([])\n",
        "                axes[i, j].set_yticklabels([])\n",
        "    \n",
        "    # Set the title of the plot\n",
        "    fig.suptitle('Images without any Human', fontsize=24)\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "fOz8TvP7hqfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create bounding boxes, crop and save as new images in new folder"
      ],
      "metadata": {
        "id": "-zVuERb9f9dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create bounding boxes\n",
        "\n",
        "# Create the new directory\n",
        "boxes_human_dir = os.path.join(original_dir, 'Human with bounding box')\n",
        "os.makedirs(boxes_human_dir, exist_ok=True)\n",
        "\n",
        "# Define the input size\n",
        "input_size = 640\n",
        "\n",
        "# Define the number of columns per row\n",
        "columns = 3\n",
        "\n",
        "# Check if there are any images in the directory, if not, skip the loop\n",
        "num_images = len(os.listdir(human_dir))\n",
        "rows = num_images // columns\n",
        "if num_images % columns != 0:\n",
        "    rows += 1\n",
        "\n",
        "# Creates a grid of subplots\n",
        "fig, axes = plt.subplots(rows, columns, figsize=(15, rows*5))\n",
        "\n",
        "# Adjusts the spacing between subplots\n",
        "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "# Convert axes to a 2D array if it is 1D\n",
        "if rows == 1 or columns == 1:\n",
        "    axes = np.array(axes).reshape(rows, columns)\n",
        "\n",
        "# Define the function to load images into NumPy arrays\n",
        "def load_image_into_numpy_array(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        im = Image.open(f)\n",
        "        return np.array(im)\n",
        "\n",
        "# Load the images into an array.\n",
        "image_paths = glob.glob(os.path.join(human_dir, '*.*'))\n",
        "image_np_array = []\n",
        "for path in image_paths:\n",
        "    image_np = load_image_into_numpy_array(path)\n",
        "    image_np_array.append(image_np)\n",
        "\n",
        "# Running inference and visualizing the results for all images\n",
        "for i, image_np in enumerate(image_np_array):\n",
        "\n",
        "    # Convert the NumPy array to a tensor with the required shape and dtype\n",
        "    input_tensor = tf.convert_to_tensor(image_np, dtype=tf.uint8)\n",
        "    input_tensor = tf.expand_dims(input_tensor, 0)\n",
        "\n",
        "    # Run inference on the input tensor\n",
        "    results = model(input_tensor)\n",
        "\n",
        "    # Convert the values to numpy arrays\n",
        "    result = {key:value.numpy() for key,value in results.items()}\n",
        "\n",
        "    # Get the indices of the detected human objects\n",
        "    human_indices = np.where(result['detection_classes'][0] == 1)[0]\n",
        "\n",
        "    label_id_offset = 0\n",
        "    image_np_with_detections = image_np.copy()\n",
        "\n",
        "    # Use keypoints if available in detections\n",
        "    keypoints, keypoint_scores = None, None\n",
        "    if 'detection_keypoints' in result:\n",
        "      keypoints = result['detection_keypoints'][0]\n",
        "      keypoint_scores = result['detection_keypoint_scores'][0]\n",
        "\n",
        "    # Draw bounding boxes\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "          image_np_with_detections,\n",
        "          result['detection_boxes'][0, human_indices],\n",
        "          (result['detection_classes'][0, human_indices] + label_id_offset).astype(int),\n",
        "          result['detection_scores'][0, human_indices],\n",
        "          category_index,\n",
        "          use_normalized_coordinates=True,\n",
        "          max_boxes_to_draw=200,\n",
        "          min_score_thresh=.5, # To filter out detections with scores below the specified threshold. \n",
        "          #Detections with scores below this threshold are considered false positives and are not shown in the output.\n",
        "          #The value is adjustable to a different threshold if you want to see more or fewer detections in the output.\n",
        "          agnostic_mode=False,\n",
        "          skip_labels=True) # To remove class name label\n",
        "    \n",
        "    # Save the image with bounding boxes in the new directory\n",
        "    img_name = os.path.basename(image_paths[i])\n",
        "    img_path = os.path.join(boxes_human_dir, img_name)\n",
        "    resized_image.save(img_path)\n",
        "\n",
        "    # Resize and pad the image to make it square\n",
        "    resized_image = resize_and_pad(image_pil, input_size, (0, 0, 0)) \n",
        "\n",
        "    # Convert the NumPy array to a PIL Image object\n",
        "    img = Image.fromarray(image_np_with_detections)\n",
        "\n",
        "    # Resize and pad the image to make it square\n",
        "    resized_image = resize_and_pad(img, input_size, (0, 0, 0))\n",
        "\n",
        "    # Calculate the row and column for this image\n",
        "    row = i // columns\n",
        "    col = i % columns\n",
        "\n",
        "    # Add the image to the plot\n",
        "    ax = axes[row, col]\n",
        "    ax.imshow(resized_image)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Set aspect ratio to square\n",
        "    ax.set_aspect('equal')\n",
        "        \n",
        "# Adjust the subplot for each row\n",
        "for i in range(rows):\n",
        "    for j in range(columns):\n",
        "        if j >= num_images % columns and i == rows - 1:\n",
        "            axes[i, j].axis('off')\n",
        "            axes[i, j].set_xticklabels([])\n",
        "            axes[i, j].set_yticklabels([])\n",
        "        \n",
        "# Set the title of the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4tKenHKfToIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crop the images according to bounding boxes and save it in new folder\n",
        "\n",
        "# Get a list of all image files in the input directory\n",
        "image_files = [file for file in os.listdir(human_dir) if file.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "# Create a new directory for cropped images\n",
        "cropped_dir = os.path.join(original_dir, 'Cropped images')\n",
        "os.makedirs(cropped_dir, exist_ok=True)\n",
        "\n",
        "target_size = 256  # Set the desired size for the cropped images\n",
        "detection_threshold = 0.5  # Set the detection threshold for displaying bounding boxes\n",
        "\n",
        "# Resizes an image to the specified size and pads the image\n",
        "def resize_and_pad_image(image_np, size):\n",
        "    height, width = image_np.shape[:2]\n",
        "\n",
        "    # Resize the image\n",
        "    if height > width:\n",
        "        scale = size / height\n",
        "    else:\n",
        "        scale = size / width\n",
        "    image_np = cv2.resize(image_np, (int(width * scale), int(height * scale)))\n",
        "\n",
        "    # Pad the image to make it square\n",
        "    max_dim = max(image_np.shape[:2])\n",
        "    top = bottom = (max_dim - image_np.shape[0]) // 2\n",
        "    left = right = (max_dim - image_np.shape[1]) // 2\n",
        "    image_np = cv2.copyMakeBorder(image_np, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "\n",
        "    return image_np\n",
        "\n",
        "# Running inference and visualizing the results for all images.\n",
        "for image_path in image_files:\n",
        "    # Load the image\n",
        "    image_np = cv2.imread(os.path.join(human_dir, image_path))\n",
        "\n",
        "    # Convert the NumPy array to a tensor with the required shape and dtype.\n",
        "    input_tensor = tf.convert_to_tensor(image_np, dtype=tf.uint8)\n",
        "    input_tensor = tf.expand_dims(input_tensor, 0)\n",
        "\n",
        "    # Run inference on the input tensor.\n",
        "    results = model(input_tensor)\n",
        "\n",
        "    # Change to NumPy array\n",
        "    result = {key: value.numpy() for key, value in results.items()}\n",
        "\n",
        "    # Get the indices of the detected human objects.\n",
        "    human_indices = np.where(result['detection_classes'][0] == 1)[0]\n",
        "\n",
        "    # Initialize the list of cropped images.\n",
        "    cropped_images = []\n",
        "\n",
        "    # Iterate over all detected humans and crop the corresponding bounding boxes.\n",
        "    for idx in human_indices:\n",
        "        # Check if the bounding box corresponds to a human (class label 1) and passes the threshold.\n",
        "        if result['detection_scores'][0, idx] >= detection_threshold:\n",
        "            ymin, xmin, ymax, xmax = result['detection_boxes'][0, idx]\n",
        "\n",
        "            # Convert the box coordinates from normalized form to pixel values.\n",
        "            im_height, im_width, _ = image_np.shape\n",
        "            ymin = int(ymin * im_height)\n",
        "            xmin = int(xmin * im_width)\n",
        "            ymax = int(ymax * im_height)\n",
        "            xmax = int(xmax * im_width)\n",
        "\n",
        "            # Draw the bounding box on the original image.\n",
        "            cv2.rectangle(image_np, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
        "\n",
        "            # Crop the image using the bounding box coordinates.\n",
        "            cropped_image = image_np[ymin:ymax, xmin:xmax, :]\n",
        "\n",
        "            # Resize and pad the cropped image to the target size.\n",
        "            image_padded = resize_and_pad_image(cropped_image, target_size)\n",
        "\n",
        "            # Save the cropped image in the new directory\n",
        "            img_name = os.path.basename(image_path)\n",
        "            img_name = os.path.splitext(img_name)[0]\n",
        "            cropped_image_path = os.path.join(cropped_dir, f\"{img_name}cropped{idx}.jpg\")\n",
        "            cv2.imwrite(cropped_image_path, image_padded)\n",
        "\n",
        "            # Append the cropped image to the list of cropped images\n",
        "            cropped_images.append(image_padded)\n",
        "\n",
        "    # Visualize the original image with bounding boxes and cropped images\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "    # Calculate the padding size for making the image square\n",
        "    im_height, im_width, _ = image_np.shape\n",
        "    max_dim = max(im_height, im_width)\n",
        "    pad_top = (max_dim - im_height) // 2\n",
        "    pad_bottom = max_dim - im_height - pad_top\n",
        "    pad_left = (max_dim - im_width) // 2\n",
        "    pad_right = max_dim - im_width - pad_left\n",
        "\n",
        "    # Add padding to the original image\n",
        "    padded_image = cv2.copyMakeBorder(image_np, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "\n",
        "# Show the padded image with bounding boxes\n",
        "    axes[0].imshow(cv2.cvtColor(padded_image, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Add empty subplots for spacing\n",
        "    axes[1].axis('off')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Draw bounding boxes on the padded image\n",
        "    for idx in human_indices:\n",
        "        if result['detection_scores'][0, idx] >= detection_threshold:\n",
        "            ymin, xmin, ymax, xmax = result['detection_boxes'][0, idx]\n",
        "            ymin = int(ymin * im_height) + pad_top\n",
        "            xmin = int(xmin * im_width) + pad_left\n",
        "            ymax = int(ymax * im_height) + pad_top\n",
        "            xmax = int(xmax * im_width) + pad_left\n",
        "            cv2.rectangle(padded_image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
        "\n",
        "    # Show the padded image with bounding boxes\n",
        "    axes[0].imshow(cv2.cvtColor(padded_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    fig.suptitle('Original image', fontsize=20, ha='right')\n",
        "    plt.show()\n",
        "\n",
        "    # Display the cropped images.\n",
        "    num_cropped_images = len(cropped_images)\n",
        "    if num_cropped_images > 0:\n",
        "        columns = 3\n",
        "        rows = (num_cropped_images + 3 - 1) // columns\n",
        "        fig, axes = plt.subplots(rows, columns, figsize=(15, rows * 5))\n",
        "        fig.suptitle('Cropped images', fontsize=20, ha='right')\n",
        "        plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
        "\n",
        "        for i, img in enumerate(cropped_images):\n",
        "            axes = axes.reshape(rows, 3)\n",
        "            row_idx = i // columns\n",
        "            col_idx = i % columns\n",
        "            ax = axes[row_idx, col_idx]\n",
        "            ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n",
        "            ax.axis('off')\n",
        "\n",
        "        # Add empty subplots to the last row if necessary.\n",
        "        if num_cropped_images % columns != 0:\n",
        "            num_empty_subplots = columns - (num_cropped_images % columns)\n",
        "            for i in range(num_empty_subplots):\n",
        "                row_idx = rows - 1\n",
        "                col_idx = num_cropped_images % columns + i\n",
        "                ax = axes[row_idx, col_idx]\n",
        "                ax.axis('off')\n",
        "\n",
        "        # Add empty subplots to the last row if necessary.\n",
        "        if num_cropped_images % columns != 0:\n",
        "            num_empty_subplots = columns - (num_cropped_images % columns)\n",
        "            for i in range(num_empty_subplots):\n",
        "                row_idx = rows - 1\n",
        "                col_idx = num_cropped_images % columns + i\n",
        "                ax = axes[row_idx, col_idx]\n",
        "                ax.axis('off')\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "u9Y3lkz771Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training new model for classification"
      ],
      "metadata": {
        "id": "Ui3XQX91mZmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "# Import the libraries as shown below\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "from glob import glob\n",
        "\n",
        "# Re-size all the images\n",
        "IMAGE_SIZE = [224, 224]\n",
        "\n",
        "train_path = '/content/drive/MyDrive/HAZIQ/data set/images/train'\n",
        "valid_path = '/content/drive/MyDrive/HAZIQ/data set/images/val'\n",
        "\n",
        "resnet = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "# Freeze the layers of the VGG16 model\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# useful for getting number of output classes\n",
        "folders = glob('/content/drive/MyDrive/HAZIQ/data set/images/train/*')\n",
        "\n",
        "# Our layers\n",
        "x = Flatten()(resnet.output)\n",
        "x = Dense(512, activation='relu')(x)  # Number of neurons in the dense layer\n",
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "\n",
        "# Create  model object\n",
        "model = Model(inputs=resnet.input, outputs=prediction)\n",
        "\n",
        "# view the structure of the model\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Use the Image Data Generator to import the images from the dataset\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "metadata": {
        "id": "9Z9YtTCd6IfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate training and validation data\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/HAZIQ/data set/images/train',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 16,\n",
        "                                                 class_mode = 'categorical')\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/HAZIQ/data set/images/val',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 16,\n",
        "                                            class_mode = 'categorical')"
      ],
      "metadata": {
        "id": "w_hlI3H7FFsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with the modified settings\n",
        "r = model.fit(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=2,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set)\n",
        ")"
      ],
      "metadata": {
        "id": "wXzxFZl8HghO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "\n",
        "# plot the accuracy\n",
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ],
      "metadata": {
        "id": "0q2HzA7nCZh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Save the model\n",
        "model.save('vgg_model.h5')"
      ],
      "metadata": {
        "id": "Jl2cM7_TOZAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('vgg_model.h5')\n"
      ],
      "metadata": {
        "id": "SHvHqk8oOWFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting the pose using new model"
      ],
      "metadata": {
        "id": "FJCOcN8kmzk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load your trained model\n",
        "model = load_model('vgg_model.h5')\n",
        "\n",
        "# List all the image files in the directory\n",
        "image_files = os.listdir(cropped_dir)\n",
        "\n",
        "# Define the class weights\n",
        "class_weights = {0: 1.0, 1: 1.5}  # Adjust the weights as per your requirement\n",
        "\n",
        "# Define the class labels\n",
        "class_labels = ['Fall', 'Sit', 'Walk']\n",
        "\n",
        "# Calculate the number of rows and columns for subplots\n",
        "num_images = len(image_files)\n",
        "num_rows = (num_images + 2) // 3  # Add 2 to account for rounding up\n",
        "num_cols = min(num_images, 3)\n",
        "\n",
        "# Create a figure and subplots\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 4*num_rows))\n",
        "\n",
        "# Iterate over the image files\n",
        "for i, image_file in enumerate(image_files):\n",
        "    # Load and preprocess the image\n",
        "    image_path = os.path.join(cropped_dir, image_file)\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image to RGB\n",
        "    img = cv2.resize(img, (224, 224))  # Resize the image to match the expected input size of your model\n",
        "    img = img / 255.0  # Normalize pixel values between 0 and 1\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    # Make predictions on the image using your trained model\n",
        "    predictions = model.predict(img)\n",
        "\n",
        "    # Get the predicted class probabilities\n",
        "    probabilities = predictions[0]\n",
        "\n",
        "    # Calculate the accuracy rates for each class\n",
        "    accuracy_rates = probabilities * 100\n",
        "\n",
        "    # Get the predicted label and accuracy rate with the highest accuracy\n",
        "    highest_accuracy_idx = np.argmax(accuracy_rates)\n",
        "    highest_label = class_labels[highest_accuracy_idx]\n",
        "    highest_accuracy = accuracy_rates[highest_accuracy_idx]\n",
        "\n",
        "    # Display the input image with the predicted label and highest accuracy rate in the corresponding subplot\n",
        "    row_idx = i // num_cols\n",
        "    col_idx = i % num_cols\n",
        "    axes[row_idx, col_idx].imshow(img[0])\n",
        "    axes[row_idx, col_idx].axis('off')\n",
        "    axes[row_idx, col_idx].set_title(f\"{highest_label}: {highest_accuracy:.2f}%\", color=(1.0, 0.0, 0.0))\n",
        "\n",
        "# Remove empty subplots if any\n",
        "for i in range(num_images, num_rows * num_cols):\n",
        "    ax = axes.flatten()[i]\n",
        "    ax.axis('off')\n",
        "\n",
        "# Show the figure\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zSfOhit2U6Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting the image using GUI"
      ],
      "metadata": {
        "id": "WbLrvlH-Rw00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import files\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Load your trained model\n",
        "model = load_model('vgg_model.h5')\n",
        "\n",
        "# Define the class labels\n",
        "class_labels = ['Fall', 'Sit', 'Walk']\n",
        "\n",
        "# Function to perform predictions on the selected image\n",
        "def predict_image():\n",
        "    # Upload an image file\n",
        "    uploaded = files.upload()\n",
        "    file_path = next(iter(uploaded))\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = cv2.imread(file_path)\n",
        "    img = cv2.resize(img, (224, 224))  # Resize the image to match the expected input size of your model\n",
        "    img = img / 255.0  # Normalize pixel values between 0 and 1\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    # Convert the image to a compatible depth\n",
        "    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "\n",
        "    # Make predictions on the image using your trained model\n",
        "    predictions = model.predict(img)\n",
        "\n",
        "    # Make predictions on the image using your trained model\n",
        "    predictions = model.predict(img)\n",
        "\n",
        "    # Get the predicted class probabilities\n",
        "    probabilities = predictions[0]\n",
        "\n",
        "    # Calculate the accuracy rates for each class\n",
        "    accuracy_rates = probabilities * 100\n",
        "\n",
        "    # Get the predicted label and accuracy rate with the highest accuracy\n",
        "    highest_accuracy_idx = np.argmax(accuracy_rates)\n",
        "    highest_label = class_labels[highest_accuracy_idx]\n",
        "    highest_accuracy = accuracy_rates[highest_accuracy_idx]\n",
        "\n",
        "    # Display the image and the label with the highest accuracy\n",
        "    plt.imshow(cv2.cvtColor(img[0], cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.title(f'{highest_label}: {highest_accuracy:.2f}%')\n",
        "    plt.show()\n",
        "\n",
        "    # Display the reupload button\n",
        "    button = widgets.Button(description=\"Reupload Image\")\n",
        "    display(button)\n",
        "\n",
        "    # Function to handle button click event\n",
        "    def reupload_image(button):\n",
        "        # Clear the previous output\n",
        "        clear_output()\n",
        "\n",
        "        # Call the predict_image function again\n",
        "        predict_image()\n",
        "\n",
        "    # Attach the button click event\n",
        "    button.on_click(reupload_image)\n",
        "\n",
        "# Call the function to perform predictions\n",
        "predict_image()\n"
      ],
      "metadata": {
        "id": "Vg1giCP0T37a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}